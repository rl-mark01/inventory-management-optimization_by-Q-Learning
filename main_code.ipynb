import gym
import numpy as np
import torch
import matplotlib.pyplot as plt

# Custom Gym Environment for Warehouse Optimization
class WarehouseEnv(gym.Env):
    def __init__(self):
        super(WarehouseEnv, self).__init__()
        
        # Define action and observation space
        self.action_space = gym.spaces.Discrete(3)  # 0: Do nothing, 1: Restock, 2: Sell
        self.observation_space = gym.spaces.MultiDiscrete([101, 10, 5])  # Inventory (0-100), Demand (0-9), Price (5 levels)
        
        # Initialize state
        self.reset()
        
    def reset(self):
        self.inventory = 50
        self.demand = np.random.randint(0, 10)
        self.price = np.random.randint(0, 5)  # 5 discrete price levels
        return self._get_state()
    
    def step(self, action):
        # Apply action
        if action == 1:  # Restock
            self.inventory = min(100, self.inventory + 10)
        elif action == 2:  # Sell
            self.inventory = max(0, self.inventory - self.demand)
        
        # Calculate reward
        if action == 2:
            reward = self.demand * (self.price * 0.2 + 0.8)  # Price levels 0-4 map to 0.8-1.6
        else:
            reward = 0
        
        # Update state
        self.demand = np.random.randint(0, 10)
        self.price = np.random.randint(0, 5)
        
        # Check if done
        done = False
        
        return self._get_state(), reward, done, {}
    
    def _get_state(self):
        return np.array([self.inventory, self.demand, self.price])

# Q-Learning Agent
class QLearningAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = 0.95
        self.learning_rate = 0.1
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.q_table = torch.zeros((*state_size, action_size), device=self.device)
        
    def get_optimal_action(self, state):
        state_tensor = torch.tensor(state, device=self.device, dtype=torch.long)
        return torch.argmax(self.q_table[state_tensor[0], state_tensor[1], state_tensor[2]]).item()
    
    def learn(self, state, action, reward, next_state):
        state_tensor = torch.tensor(state, device=self.device, dtype=torch.long)
        next_state_tensor = torch.tensor(next_state, device=self.device, dtype=torch.long)
        
        current_q = self.q_table[state_tensor[0], state_tensor[1], state_tensor[2], action]
        max_next_q = torch.max(self.q_table[next_state_tensor[0], next_state_tensor[1], next_state_tensor[2]])
        new_q = current_q + self.learning_rate * (reward + self.gamma * max_next_q - current_q)
        self.q_table[state_tensor[0], state_tensor[1], state_tensor[2], action] = new_q

# Initialize environment and agent
env = WarehouseEnv()
state_size = env.observation_space.nvec
action_size = env.action_space.n
agent = QLearningAgent(state_size, action_size)

# Training parameters
episodes = 10000
epsilon = 1.0  # 초기 epsilon은 1.0
epsilon_min = 0.01
epsilon_decay = 0.999  # 감소율을 작게 설정

# Training loop
for e in range(episodes):
    state = env.reset()
    total_reward = 0

    for time in range(200):  # Longer episode length
        # Choose action based on epsilon-greedy policy
        if np.random.rand() <= epsilon:
            action = np.random.randint(action_size)  # Random action
        else:
            action = agent.get_optimal_action(state)  # Exploit learned policy

        next_state, reward, done, _ = env.step(action)

        # Reward scaling and penalty
        if action == 2 and state[0] == 0:
            reward -= 10  # Penalty for no inventory
        elif action == 1 and state[0] > 90:
            reward -= 5  # Penalty for overstocking

        agent.learn(state, action, reward, next_state)  # Update Q-table
        state = next_state
        total_reward += reward

        if done:
            break

    # Decay epsilon (after some initial episodes)
    if e > 500 and epsilon > epsilon_min:
        epsilon *= epsilon_decay

    # Log progress
    rewards_history.append(total_reward)
    if (e + 1) % 100 == 0:
        print(f"Episode: {e + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.4f}")


# Visualization of training progress
plt.figure(figsize=(10, 5))
plt.plot(rewards_history)
plt.title('Rewards per Episode')
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.show()

# Test the trained model
test_episodes = 10
for e in range(test_episodes):
    state = env.reset()
    total_reward = 0
    inventory_history = []
    demand_history = []
    price_history = []

    for time in range(100):
        action = agent.get_optimal_action(state)
        next_state, reward, done, _ = env.step(action)
        state = next_state
        total_reward += reward

        inventory_history.append(state[0])
        demand_history.append(state[1])
        price_history.append(state[2])

        if done:
            break

    print(f"Test Episode: {e + 1}/{test_episodes}, Total Reward: {total_reward}")

    # Visualization for each test episode
    plt.figure(figsize=(15, 5))
    plt.subplot(1, 3, 1)
    plt.plot(inventory_history)
    plt.title('Inventory Level')
    plt.xlabel('Time Step')
    plt.ylabel('Inventory')

    plt.subplot(1, 3, 2)
    plt.plot(demand_history)
    plt.title('Demand')
    plt.xlabel('Time Step')
    plt.ylabel('Demand')

    plt.subplot(1, 3, 3)
    plt.plot(price_history)
    plt.title('Price Level')
    plt.xlabel('Time Step')
    plt.ylabel('Price')

    plt.tight_layout()
    plt.show()
